{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER for bsnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение задачи `NER` на преобразованных данных `bsnlp`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключаем нужные библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corus\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertModel, BertPreTrainedModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем преобразованные под задачу `NER` данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/datasets/bsnlp/bsnlp_train_processed.csv\", encoding=\"utf-8\").fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_idxs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>clidxs</th>\n",
       "      <th>base_forms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ru-110</td>\n",
       "      <td>Азия Норин , известная как Азия Биби восемь ле...</td>\n",
       "      <td>B_PER I_PER O O O B_PER I_PER O O O O</td>\n",
       "      <td>B_PER-Asia-Bibi I_PER-Asia-Bibi O O O B_PER-As...</td>\n",
       "      <td>Азия Норин O O O Азия Биби O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ru-110</td>\n",
       "      <td>Теперь она планирует покинуть Пакистан ва исла...</td>\n",
       "      <td>O O O O B_LOC O O O O</td>\n",
       "      <td>O O O O B_GPE-Pakistan O O O O</td>\n",
       "      <td>O O O O Пакистан O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ru-110</td>\n",
       "      <td>Верховный суд в Исламабаде признал ее невиновн...</td>\n",
       "      <td>B_ORG I_ORG I_ORG I_ORG O O O O O O O O O O O ...</td>\n",
       "      <td>B_ORG-Supreme-Court-of-Pakistan I_ORG-Supreme-...</td>\n",
       "      <td>Верховный суд в Исламабаде O O O O O O O O O O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ru-110</td>\n",
       "      <td>Под суд Норин в 2010 году после ссоры с соседя...</td>\n",
       "      <td>O O B_PER O O O O O O O O</td>\n",
       "      <td>O O B_PER-Asia-Bibi O O O O O O O O</td>\n",
       "      <td>O O Норин O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ru-110</td>\n",
       "      <td>Мусульманки , вместе с которыми она собирала ф...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_idxs                                             tokens  \\\n",
       "0   ru-110  Азия Норин , известная как Азия Биби восемь ле...   \n",
       "1   ru-110  Теперь она планирует покинуть Пакистан ва исла...   \n",
       "2   ru-110  Верховный суд в Исламабаде признал ее невиновн...   \n",
       "3   ru-110  Под суд Норин в 2010 году после ссоры с соседя...   \n",
       "4   ru-110  Мусульманки , вместе с которыми она собирала ф...   \n",
       "\n",
       "                                                tags  \\\n",
       "0              B_PER I_PER O O O B_PER I_PER O O O O   \n",
       "1                              O O O O B_LOC O O O O   \n",
       "2  B_ORG I_ORG I_ORG I_ORG O O O O O O O O O O O ...   \n",
       "3                          O O B_PER O O O O O O O O   \n",
       "4          O O O O O O O O O O O O O O O O O O O O O   \n",
       "\n",
       "                                              clidxs  \\\n",
       "0  B_PER-Asia-Bibi I_PER-Asia-Bibi O O O B_PER-As...   \n",
       "1                     O O O O B_GPE-Pakistan O O O O   \n",
       "2  B_ORG-Supreme-Court-of-Pakistan I_ORG-Supreme-...   \n",
       "3                O O B_PER-Asia-Bibi O O O O O O O O   \n",
       "4          O O O O O O O O O O O O O O O O O O O O O   \n",
       "\n",
       "                                          base_forms  \n",
       "0                 Азия Норин O O O Азия Биби O O O O  \n",
       "1                           O O O O Пакистан O O O O  \n",
       "2  Верховный суд в Исламабаде O O O O O O O O O O...  \n",
       "3                          O O Норин O O O O O O O O  \n",
       "4          O O O O O O O O O O O O O O O O O O O O O  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B_LOC': 0,\n",
       " 'I_ORG': 1,\n",
       " 'I_PER': 2,\n",
       " 'I_LOC': 3,\n",
       " 'I_EVT': 4,\n",
       " 'B_PRO': 5,\n",
       " 'B_ORG': 6,\n",
       " 'O': 7,\n",
       " 'I_PRO': 8,\n",
       " 'B_EVT': 9,\n",
       " 'B_PER': 10,\n",
       " 'PAD': 11}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_values = list(set(' '.join(data['tags'].values).split()))\n",
    "tag_values.append('PAD')\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 75\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('/tokenizer_ru/', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split()):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B_PER I_PER O O O B_PER I_PER O O O O'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0]['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['tokens'].values\n",
    "labels = data['tags'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(sentences, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `dataloader`'а:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataloader(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, labels, MAX_LEN):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.MAX_LEN = MAX_LEN\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.text[index]\n",
    "        input = torch.zeros(MAX_LEN, dtype=torch.long)\n",
    "        ids = tokenizer.convert_tokens_to_ids(['[CLS]'] + sentence + ['[SEP]'])\n",
    "        input[:len(ids)] = torch.LongTensor(ids)\n",
    "        \n",
    "        mask = torch.LongTensor([int(i != 0.0) for i in input])\n",
    "        \n",
    "        taget = torch.zeros(MAX_LEN, dtype=torch.long)\n",
    "        label = self.labels[index]\n",
    "        label = [tag2idx['PAD']] + [tag2idx.get(l) for l in label] + [tag2idx['PAD']]\n",
    "        taget[:len(label)] = torch.LongTensor(label)\n",
    "        return input,mask, taget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем до этого загруженные и токенизированные даннные на train и valid и передаем `dataloader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(tokenized_texts, labels,\n",
    "                                                            random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader =  torch.utils.data.DataLoader(NerDataloader(X_train, Y_train, MAX_LEN), batch_size=16,\n",
    "                                               shuffle=True, num_workers=6)\n",
    "\n",
    "val_dataloader =  torch.utils.data.DataLoader(NerDataloader(X_val, Y_val, MAX_LEN), batch_size=16,\n",
    "                                               shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask, label = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим саму модель, в данном случае в качестве feature extractor берется `RuBERT` от (deeppavlov):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSoftmaxForNer(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_dropout_prob=768, dropout = 0.1):\n",
    "        super(BertSoftmaxForNer, self).__init__()\n",
    "        self.num_labels = num_classes\n",
    "        self.bert = BertModel.from_pretrained('/bert_ru/')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_dropout_prob, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids = input_ids,attention_mask=attention_mask,token_type_ids=None)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем модель, оптимизатор и т.д.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertSoftmaxForNer(len(tag2idx)).cuda()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tag2idx['PAD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение и результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем обучение с выводом `accuracy` и `f1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:13<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.15201487969898153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/128 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.991792891195462\n",
      "Validation F1-Score: 0.9931423561574096\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:12<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.021506608061827137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/128 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9948102106088951\n",
      "Validation F1-Score: 0.9957354833724985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:12<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.011972891865298152\n",
      "Validation Accuracy: 0.9959567919859996\n",
      "Validation F1-Score: 0.9963425696340653\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "        loss = criterion(logits.view(logits.size(0)*logits.size(1),-1), b_labels.view(logits.size(0)*logits.size(1)))\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    torch.save(model.state_dict(), '/weights/weights' + str(epoch) + '.pht')\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask)\n",
    "        logits = outputs[0].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "    print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags,average='weighted')))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получившиеся результаты по различным тегам в виде `precision`, `recall` и `f1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B_EVT       0.85      0.87      0.86        39\n",
      "       B_LOC       1.00      1.00      1.00     13173\n",
      "       B_ORG       0.85      0.95      0.90        73\n",
      "       B_PER       0.96      0.97      0.97       187\n",
      "       B_PRO       0.92      0.67      0.77        18\n",
      "       I_EVT       1.00      0.30      0.46        20\n",
      "       I_LOC       0.00      0.00      0.00         5\n",
      "       I_ORG       0.86      0.83      0.85        53\n",
      "       I_PER       0.94      0.96      0.95        69\n",
      "       I_PRO       0.90      1.00      0.95         9\n",
      "           O       0.99      1.00      0.99      2925\n",
      "\n",
      "    accuracy                           1.00     16571\n",
      "   macro avg       0.84      0.78      0.79     16571\n",
      "weighted avg       1.00      1.00      1.00     16571\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/easemerova/.conda/envs/my_py/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/easemerova/.conda/envs/my_py/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/easemerova/.conda/envs/my_py/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_tags, pred_tags))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
